Тема 3. Работайте как Southbridge. Технический регламент

1. Основы настройки выделенных серверов

Как упоминалось в предыдущих докладах в нашей компании автоматизированы и стандартизированы многие процессы, настройка серверов и их тюнинг не исключение. Мы поговорим о том как происходит настройка, на какие моменты стоит обращать внимание при настройке и как выжать из сервера максимум его возможностей.

Здесь начать стоит с так называемой “Первичной настройки”. Под первичной настройкой мы подразумеваем процесс переноса проекта клиента на сервер полностью настроенный по нашим стандартам.

Почему первичная настройка важна? И зачем трогать то, что работает?

В начале работы с Southbridge многие клиенты спрашивают, зачем нужна первоначальная настройка. Их интересует, зачем трогать то, что работает? Почему необходимо перенастраивать сервер, на котором выполняются критичные для клиента приложения? Ответ прост: чтобы в дальнейшем использовать все преимущества и научить сервер работать с максимальной отдачей.

Преимущества которые дает первичная настройка:

* При первичной настройке производится установка пакетов и версий серверного ПО, которое уже проверено на сотнях серверов других наших клиентов.
* Подключаются средства мониторинга, которые будут работать в хорошо протестированной конфигурации с оптимальными настройками.
* Сервер проверяется на наличие руткитов и другого вредоносного ПО, о существовании которого вы можете даже не догадываться.
* Стандартизируются пути расположения проектов и файлов конфигурации, что упрощает поиск проблемы в случае её возникновения и сокращает время на ее устранение.
* Процесс создания и настройки площадок под новые сайты занимает минуты.
* Приведение используемого ПО к единому стандарту позволяет упростить процесс обновления пакетов.
* Параллельно выполняется тонкая оптимизация конфигурации сервера.

Всё это обеспечивает максимальную доступность проектов и сводит к минимуму временные затраты. А система создания бекапов надёжно защищает серверы от потерь данных.

Как производится первичная настройка

1. Настраивается временный сервер на который переносится проект.
2. После проверки со стороны клиента договариваемся, в какое время можно выполнить чистовую синхронизацию данных с минимальным даунтаймом. При необходимости провести работы в то время, когда целевая аудитория наименее активна.
3. После перевода трафика на временный сервер переустанавить с нуля операционную систему и настраивается виртуализация. Далее выполняется черновое копирование виртуальной среды временного сервера на основной сервер.
4. После согласования выполняется чистовая синхронизация на основной сервер и переводится трафик на новый сервер.

Перейдем к самому процессу настройки сервера.

Технические требования

Перед установкой важно проверить правильность сборки RAID!
Для серверов под веб оптимально использовать RAID 1, для серверов БД 4 диска в RAID 10 (в идеале аппаратный RAID-10).

Максимум выделяем места для корня. В конце жесткого диска выделяем порядка 8-16 Gb для SWAP.
На слайде показан пример разбивки для серверов с OpenVZ

Для более сложных клиентов, в частности на Kubernetes используется более сложная структура и задействуется LVM.

При заказе сервера обязательно устанавливаем минимальные версии ОС которые не содержат лишний софт.

Что бы получить уведомления от хостинг провайдера в дополнительных адресах указывается email адрес группы которая будет обслуживать проект, письма отправленные на этот адрес будут трансформироваться в задачи в Redmine.

В задаче Redmine по настройке сервера обязательно используется отдельный трекер: Первичка, так мы можем быстро отследить сколько серверов в текущий момент находится на настройке, а также кто настраивал сервер в прошлом.

К каждой задаче по настройке обязательно прикрепляется чек-лист который называется “Первоначальная настройка физического сервера” со списком вещей которые необходимо обязательно выполнить (это было сделано для удобства администраторов, т.к. без этого списка довольно часто забывали выполнить обязательные пункты настройки).

Обязательно прописываем обратные PTR-записи, делаем либо самостоятельно, если панель управления хостингом это позволяет либо через запрос в службу поддержки, если такой функции в панели нет. 
PTR запись - если объяснять простыми словами, это запись, которая связывает IP адрес с доменом вашего сайта/сервера.
У вашего домена на DNS серверах прописана А запись, которая связывает ваш домен с IP адресом сервера, где расположен ваш сайт. А PTR запись делает наоборот - связывает IP адрес с доменом.

PTR запись в основном необходима при отправке писем от имени вашего домена. Большинство почтовых серверов, прежде чем принять решение - поместить письмо во "Входящее" или отклонить как СПАМ, проверяют запись PTR у IP адреса сервера, с которого это письмо пришло. Если данная запись есть, и она совпадает с именем домена, от имени которого пришло письмо, то это будет являться одним из фактов для принятия положительного решения (что это письмо не СПАМ). 
Если записи нет, или она не совпадает с доменом - почтовый сервер будет более внимательно проверять письмо по другим критериям.

Следующим шагом прописываем hostname сервера, здесь также имеются свои стандарты:
Физические серверы мы именуем как ds.example.com, виртуальные серверы именуем как vs.example.com
Такая структура подходит большинству клиентов и отлично работает когда количество физических и виртуальных серверов небольшое.
Для крупных клиентов, где в работе десятки серверов мы используем более осмысленные имена которые обычно даются по ролям которые данный сервер выполняет, например mysql-a.example.com kube-master.example.com и так далее

Регистрация сервера в Slack

Абсолютно каждый физический или виртуальный сервер должен быть подключен к slack, о slack и как он рассказывал я рассказывал в предыдущем докладе.

После регистрации в Slack необходимо зарегистрировать клиента в Zabbix (в случаях если клиент новый). Для этого у нас есть самописный скрипт на Python, который выполняет регистрацию клиента в Zabbix, подготавливает все необходимые структуры и возвращает логин/пароль который клиент сможет использовать для просмотра графиков и статистики по своим серверам.

Далее идет работа с пакетами.

Обязательно выполняем полное обновление всех пакетов и ядра до последних стабильных версий и перезагружаем сервер. Устанавливаем репозитории epel и Southbridge. У компании есть отдельный репозиторий в котором мы храним многие из необходимых нам пакетов, в том числе собранных самостоятельно, а также шаблоны виртуальных машин, подробнее про сборку пакетов мы расскажем в одной из следующих тем. Ссылку на репозиторий вы можете видеть на слайде

Устанавливаем пакет slack и запускаем сам slack. Slack выполнит всю необходимую настройку основываясь на списке ролей которые мы ему предоставили, в том числе полная настройка OpenVZ и установка ядра OpenVZ. Поскольку OpenVZ использует собственное ядро после того как slack отработает нам необходимо перезагрузить сервер что бы загрузиться с нового ядра которое slack так любезно установил. После того как сервер поднимется удаляем старое ядро.

Инициализация smartmontools

Выполняется очень просто, команды вы можете видеть на слайде

Для тех кто не знает что такое smartmontools. Это набор из двух утилит которые контролируют и следят за системными накопителями используя самопроверку, анализ и систему технологического отчёта (S.M.A.R.T.), встроенную в большинство современных дисков.

Устанавливаем параметры разделов.

Добавляем параметр noatime для корневого и /vz раздела, в файле /etc/fstab

Параметр noatime полностью отключает запись времени доступа к файлу. Это может улучшить производительность доступа к данным. Большинство программ не используют это поле.

Последним шагом настраивается резервное копирование
Для настройки необходимо сгенерировать ssh-ключ, под пользователем root

На сервере бэкапов под каждый проект выделен отдельный пользователь, данному пользователю прописывается сгенерированный ключ по которому он будет подключаться. Это все действия которые необходимо выполнить что бы бэкапы заработали, все остальное за нас делает система управления конфигурациями.

Для снятия сервера с поддержки у нас так же есть специальный самописный скрипт, после его запуска он удаляет слак, наших пользователей, ограничение доступа по IP к SSH и так далее. На выходе мы получаем пароль от root пользователя который передаем клиенту 

2. Основы работы с OpenVZ/KVM. Установка, настройка, эксплуатация

В данной теме мы поговорим о том как работать с контейнерами OpenVZ и виртуальными машинами KVM.

OpenVZ — реализация технологии виртуализации на уровне операционной системы, которая базируется на ядре Linux. OpenVZ позволяет на одном физическом сервере запускать множество изолированных копий операционной системы, называемых «виртуальные частные серверы» (Virtual Private Servers, VPS) или «виртуальные среды» (Virtual Environments, VE).

Поскольку OpenVZ базируется на ядре Linux, в отличие от виртуальных машин (например KVM), в роли «гостевых» систем могут выступать только дистрибутивы Linux. Однако, виртуализация на уровне операционной системы в OpenVZ даёт лучшую производительность, масштабируемость, плотность размещения, динамическое управление ресурсами, а также лёгкость в администрировании, чем у альтернативных решений. Согласно сайту OpenVZ, накладные расходы на виртуализацию очень малы, и падение производительности составляет всего 1-3 %, по сравнению с обычными Linux-системами.

Виртуализация и изоляция

Каждый контейнер — это отдельная сущность, и с точки зрения владельца контейнера она выглядит практически как обычный физический сервер. 

Каждый контейнер имеет свои собственные:

* Файлы: Системные библиотеки, приложения, виртуализованные файловые системы, виртуализованные блокировки и так далее
* Собственные пользователи и группы: включая root.
* Собственное дерево процессов: контейнер видит только свои собственные процессы (начиная с init). Идентификаторы процессов (PID) также виртуализованы, поэтому PID программы init — 1.
* Собственную сеть: Виртуальное сетевое устройство (venet), позволяющая контейнеру иметь свои собственные адреса IP, а также наборы правил маршрутизации и файрволла.
* Собственные устройства: При необходимости администратор OpenVZ сервера может дать контейнеру доступ к реальным устройствам, напр. сетевым адаптерам, портам, разделам диска и так далее
* Собственные объекты межпроцессорного взаимодействия: Разделяемая память, семафоры, сообщения.

Управление ресурсами

Управление ресурсами в OpenVZ состоит из трёх компонентов: двухуровневая дисковая квота, честный планировщик процессора, и так называемые «юзер бинкаунтеры» (user beancounters). Эти ресурсы могут быть изменены во время работы контейнера, перезагрузка не требуется.

Двухуровневая дисковая квота

Администратор OpenVZ сервера может установить дисковые квоты на контейнер, в терминах дискового пространства и количества айнодов. Это первый уровень дисковой квоты.
В дополнение к этому, администратор контейнера может использовать обычные утилиты внутри своего контейнера для настроек стандартных дисковых квот UNIX для пользователей и групп

Честный планировщик процессора

Планировщик процессора в OpenVZ также двухуровневый. На первом уровне планировщик решает, какому контейнеру дать квант процессорного времени, базируясь на значении параметра cpuunits для контейнера. На втором уровне стандартный планировщик Linux решает, какому процессу в выбранном контейнере дать квант времени, базируясь на стандартных приоритетах процесса в Linux.

Администратор OpenVZ сервера может устанавливать различные значения cpuunits для разных контейнеров, и процессорное время будет распределяться соответственно соотношению этих величин.

Также имеется параметр ограничения — cpulimit, устанавливающий верхний лимит процессорного времени в процентах, отводимый для определенного контейнера.

Возможно у многих возникнет вопрос почему OpenVZ и зачем вообще его использовать в 2018 году. Каждый инструмент выбирается под конкретную задачу, с теми задачами для которых мы используем OpenVZ он отлично справляется. Это небольшие клиенты у которых обычно 1-2 сервера и несколько сайтов на PHP/Ruby/Python или чём-то еще, либо веб-студии у которых по несколько десяток сайтов в нескольких контейнерах. Большинство из них не выдвигают каких-то особых требований, обычно это стандартный LAMP стэк который себя прекрасно чувствует в контейнерах, простота в данном случае побеждает. 

Масштабируемость

Ввиду того, что OpenVZ использует одно ядро linux для всех контейнеров, система является столь же масштабируемой, как и обычное ядро Linux версии 2.6, то есть поддерживает максимально до 4096 ядер процессора и до 64ТБ RAM. Единственный контейнер может быть расширен до размеров всего физического сервера, то есть, использовать все доступное процессорное время и память.

Можно использовать OpenVZ с единственным контейнером на сервере. Данный подход позволяет контейнеру полностью использовать все аппаратные ресурсы сервера с практически «родной» производительностью, и пользоваться дополнительными преимуществами: независимость контейнера от «железа», подсистему управления ресурсами, миграцию между серверами и так далее.

Плотность

OpenVZ способна размещать сотни контейнеров на современных серверах. Основными ограничивающими факторами являются объём памяти и частоты процессора

Перейдем к созданию нового OpenVZ контейнера.

Как я упоминал в предыдущей теме у нас есть готовые шаблоны контейнеров на все случаи жизни поэтому установка не занимает много времени. К примеру нам нужно развернуть контейнер в котором в дальнейшем будут развернуты сайты на PHP 5.6 и MySQL. Для этого возьмем за основу уже существующий шаблон

Скачиваем шаблон на физический сервер и распакуем его

Перемещаем файлы шаблона на свои места, попутно изменив ID контейнера на уникальный

В конфиг файле контейнера необходимо поменять  два параметра IP адрес по которому он будет доступен и его хостнейм.

Создаем root директорию контейнера и запускаем его

Регистрируем контейнер в слак, и запускаем slack который также выполнит настройку в соответствии со списком указанных ролей.
Обновляем все пакеты до последних стабильных версий и перезапускаем контейнер, на этом настройка контейнера завершена.

Теперь поговорим об эксплуатации только что созданного контейнера, комплект пользовательских утилит для OpenVZ состоит из:

vzctl — Основная утилита для управления контейнерами. Применяется для создания, удаления, запуска, остановки и перезапуска контейнеров, а также для применения новых параметров, стоит отметить что большинство параметров можно применить на работающем контейнере без необходимости перезагрузки.

Примеры команд вы можете видеть на слайде

Здесь мы создаем новый контейнер на базе шаблона CentOS 7 Minimal, назначаем контейнеру IP адрес, устанавливаем nameserver и запускаем контейнер.

Следующая команда запустит uptime внутри контейнера и вернет результат

Также мы можем использовать контейнер как обычный виртуальный сервер для этого выполняем команду  vzctl enter
Результат будет таким же как если бы вы подключились к виртуальному серверу по SSH, для того чтобы выйти из контейнера введите exit или Ctrl+D

Для остановки контейнера используется команда stop, кто бы мог подумать!

Что бы удалить контейнер используем команду destroy

vzlist — Печатает листинг контейнеров. Поддерживает выборки по различным параметрам контейнеров и их комбинациям.

Также очень полезная команда, с ее помощью мы можем посмотреть список контейнеров на сервере.

Что бы посмотреть запущенные контейнеры достаточно просто выполнить vzlist

У команды есть несколько полезных аргументов это -а (минус а) что бы посмотреть все контейнеры и -S (минус S) чтобы посмотреть остановленные контейнеры

vzmigrate — Утилита для offline и online миграции. 

vzcfgvalidate — Проверяет верность конфигурационного файла контейнера.

Если вы правите конфиг файл руками а не через vzctl неплохо было бы перепроверить за собой результат с помощью данной утилиты, если в файле есть ошибки вы об этом сразу узнаете

vzmemcheck, vzcpucheck, vzcalc — Служат для получения информации об используемых ресурсах.

vzsplit — Служит для генерации конфигурационных файлов контейнеров. Позволяет «разделить» физический сервер на указанное число частей. Он генерирует полный набор параметров управления системными ресурсами для заданного количества контейнеров. Значения рассчитываются из общей физической памяти сервера, на котором запущена утилита, и количества контейнеров, которые должен иметь сервер, даже если указанное количество контейнеров потребляет все доступные ресурсы.

vzpid — Позволяет определить, какому контейнеру принадлежит процесс. В качестве параметра принимает pid-номер процесса. Если с физического сервера запустить например команду top то помимо процессов самого физического сервера мы увидим также все процессы всех контейнеров, данная команда бывает очень полезна когда необходимо установить источник повышенной нагрузки, допустим в top мы видим что какой-то процесс потребляет чрезмерно большое количество ресурсов, но мы не знаем в каком контейнере он запущен, берем его PID и запускаем команду vzpid c PID в качестве аргумента, результатом будет ID контейнера.

vzquota — Утилита для работы с дисковой квотой контейнера. 

vzubc — Утилита для замены юзер бинкаунтеров. 

Как вы можете видеть работа с OpenVZ контейнерами является довольно простой задачей, и разобраться во всем этом можно достаточно хорошо всего за пару часов. На этом мы закончим наш обзор OpenVZ и перейдем к виртуальным машинам на базе KVM.

KVM

Переходим к технологии KVM которая расшифровывается как (Kernel-based Virtual Machine), которую создала компания RedHat, и которая имеет открытый исходный код, являясь бесплатной альтернативой своих коммерческих аналогов, например VMWare. 

Глоссарий

Сначала немного поговорим о том, как работает KVM. Ничего заумного, просто небольшое введение, чтобы вы знали базовую терминологию.

KVM использует технологию аппаратной виртуализации, поддерживаемую современными процессорами от Intel и AMD. Используя загруженный в память модуль ядра KVM, с помощью драйвера пользовательского режима (который представляет собой модифицированный драйвер от QEMU), эмулирует слой аппаратного обеспечения, поверх которого могут создаваться и запускаться виртуальные машины. KVM может функционировать и без аппаратной виртуализации (если она не поддерживается процессором) но в этом случае, она работает в режиме чистой эмуляции с использованием QEMU и производительность виртуальных машин очень сильно снижается.

Для управления KVM можно использовать графическую утилиту, похожую на продукты от VMware и VirtualBox, а также командную строку.

Самым популярным графическим интерфейсом является Virtual Machine Manager (VMM), созданный в RedHat. Он также известен по имени своего пакета как virt-manager и содержит несколько утилит, включая virt-install, virt-clone, virt-image и virt-viewer, служащие для создания, клонирования, установки и просмотра виртуалльных машин соответственно.

Базовый интерфейс командной строки KVM обеспечивается утилитой virsh. В определенных случаях вы можете использовать утилиты поддержки, такие как virt-install, для создания своих виртуальных машин.

Преимущества и недостатки KVM

Как и любое программное решение, KVM имеет как плюсы, так и минусы, исходя из которых, хостеры и конечные потребители принимают решение об использовании данного решения.

Основными преимуществами гипервизора являются:

* Независимо распределяемые ресурсы. Каждая работающая под управлением KVM виртуальная машина получает свой объем оперативной и постоянной памяти и не может «залезть» в другие области, что повышает стабильность работы;
* Широкая поддержка гостевых операционных система. Кроме полной поддержки UNIX-дистрибутивов, в том числе *BSD, Solaris, Linux, есть возможность устанавливать Windows и даже MacOS;
* Взаимодействие с ядром позволяет напрямую обращаться к оборудованию рабочей станции, что делает работу более быстрой;
* Наличие финансовой поддержки со стороны таких компаний как (RedHat, HP, Intel и IBM) позволяет проекту быстро развиваться, охватывая все большее количество оборудования и операционных систем, в том числе новейших;
* Простое администрирование – возможность удаленного управления через VNC и большое количество стороннего ПО и надстроек.

Без недостатков также не обошлось:

* Относительная молодость гипервизора и соответственно взрывной рост приводят к различным проблемам, особенно при добавлении поддержки нового оборудования и программного окружения;
* Сложность настроек, особенно для неискушенного пользователя. Правда, большинство опций можно не менять – они настроены оптимально «из коробки».

Функциональные возможности и свойства гипервизора

Комплекс KVM характеризуется такими основными свойствами – безопасность, удобное управление памятью, надежное хранение данных, динамическая миграция, производительность, масштабируемость и стабильность.

Безопасность

В KVM каждая машина представляет собой Linux-процесс, потому на нее автоматически распространяются стандартные политики безопасности, а также изоляция от других процессов. Специальные надстройки (такие как SELinux) добавляют и другие элементы безопасности – контроль доступа, шифрование и т.д.

Управление памятью

Поскольку KVM является частью ядра Linux, гипервизор наследует мощные инструменты управления памятью. Страницы памяти каждого процесса, т.е. виртуальных машин, могут быстро копироваться и меняться без ущерба скорости работы. С поддержкой многопроцессорных систем KVM получила возможность управлять большими объемами памяти. Поддерживается также обобщение памяти – объединение одинаковых страниц и выдача копии машине по запросу, а также другие методы оптимизации.

Хранение данных

Для хранения образов машин и их данных KVM может использовать любые носители, которые поддерживаются хостовой операционной системой – жесткие диски, NAS, съемные накопители, в том числе с многопоточным вводом-выводом для ускорения работы. Кроме того, гипервизор может работать с распределёнными файловыми системами – например, Ceph. Диски для KVM имеют собственный уникальный формат, который поддерживает динамическое создание снимков разного уровня, шифрование и сжатие.

Динамическая миграция

Важная особенность KVM – поддержка динамической миграции: перемещение виртуальных машин между различными хостами без их остановки. Для пользователей такая миграция совершенно незаметна – машина остается рабочей, производительность не страдает, сетевые соединения активны. Конечно, возможна и миграция через сохранение текущего состояния виртуалки в снимок и развертывание на другом хосте.

Производительность и масштабируемость

Масштабируемость и производительности комплекса благодаря плотной интеграции с Linux, полностью унаследованы от ядра. Таким образом, гипервизор поддерживает до 16 процессоров (виртуальных или физических) и до 256 ГБ ОЗУ в каждой виртуальной машине. Это позволяет использовать гипервизор даже в наиболее высоконагруженных системах.

Теперь перейдем непосредственно к подготовке хоста  и развертывании  гостевой системы

Установку, настройку и создание виртуальных машин мы полностью автоматизировали с помощью нескольких Ansible ролей, их вы можете найти в репозитории red slurm в разделе практика

Дисковая подсистема хоста

RAID

Чтобы получить нормальную производительность дисковой подсистемы для многих виртуальных машин, используется аппаратный RAID-контроллер с кэшем. Если SSD, то можно и программный. Лучше 10 уровня.

Подготовка разделов

Используем LVM. /boot выносим на отдельную primary partition. Схему разделов вы можете видеть на слайде

После того как мы подготовили физический сервер переходим непосредственно к настройке хоста и созданию виртуальных машин.

Первое что нам необходимо сделать скачать 3 роли bridge, nat и kvm
Затем открываем host_vars  файл конкретного сервера и указываем список ролей

Через переменные указываем конфигурацию моста. Мост или как его еще называют Бридж — это способ соединения двух сегментов Ethernet на канальном уровне, т.е. без использования протоколов более высокого уровня, таких как IP. Пакеты передаются на основе Ethernet-адресов, а не IP-адресов (как в маршрутизаторе). Поскольку передача выполняется на канальном уровне (уровень 2 модели OSI), все протоколы более высокого уровня прозрачно проходят через мост.

Вписать в эти переменные сетевые настройки сервера. bridge_address_peer указывать только в том случае, когда это нужно (например, в Hetzner адреса на интерфейсах имеют префикс 32 и также на интерфейсе указывается peer, он же шлюз по-умолчанию). Метрику маршрута по умолчанию указать бо'льшую, чем есть на момент начала конфигурирования. Это важно, иначе неизбежна потеря управления. Если нет никакой - то поставить, к примеру 10.
br0 - мост в публичной сети, br1 - в приватной. Если приватной сети ("локалки") нет - то мост br1 создавать не нужно. Но это только в том случае, если всем ВМ планируется присваивать "белые" адреса (в этом случае роль 'nat' можно не использовать), в остальных случаях "локалка" нужна.
Если ваш сервер не имеет физических интерфейсов в приватной сети, то оставьте соответствующий список bridge_ports: [] пустым
При выполнении будут добавлены и сконфигурированы на уровне 3 интерфейсы мостов, затем с указанных в 'bridge_ports' интерфейсов Ethernet будет убрана конфигурация 3 уровня, и они будут включены в мосты. После этого (если сохранится управление) будут записаны файлы конфигурации сети.

Конфигурация трансляции адресов

Тут, наверное, всё понятно. 'nat_network' - серая сеть, которую будем натить, 'nat_interface' - исходящий интерфейс, 'nat_ip_address' - адрес, подставляемый при трансляции (адрес исходящего интерфейса).

Конфигурация доступа 

Тут указать серый адрес сервера (при его наличии).

Конфигурация пула хранения

Параметры вы можете видеть на слайде
Пулов можно указать любое количество.

Конфигурация виртуальных машин

В качестве 'kvm_vm_name' указать доменное имя третьего уровня (если FQDN у нас 'guestname.example.com' - то нужно указать 'guestname'. При первичной конфигурации ВМ её FQDN составляется из значения данной переменной с прибавлением доменов .второго.первого уровня из FQDN хостовой системы. Всё остальное достаточно очевидно.

Конфигурация гостевых систем

После развёртывания гостевой системы ролью kvm при первом запуске на ней выполняется скрипт, устанавливающий некоторые репозитории, файлы и сервисы. После этого виртуальный сервер превращается в полноценный хост, на котором можно выполнить первичную настройку и установку всех необходимых сервисов.

Мы рассмотрели как работает KVM и как производить его установку и настройку с помощью Ansible. 
Но как бы ни были полезны средства автоматизации от ручных задач все равно никуда не деться, поэтому я расскажу о ручном управлении виртуальными серверами, мы рассмотрим основные команды.

Список подкоманд команды virsh вы можете видеть на слайде. Как видно из списка с помощью virsh можно сделать практически все, давайте поподробнее разберем наиболее полезные команды.

3. Работа с аппаратными RAID-контроллерами

В данной теме мы постараемся разобраться что из себя представляет технология RAID, а так же научимся работать с некоторым из самых распространенных аппаратных RAID-контроллеров, это RAID-контроллеры Adaptec и LSI MegaRAID.

RAID (или избыточный массив независимых дисков) — технология виртуализации данных, которая объединяет несколько дисков в логический элемент для избыточности и повышения производительности.

RAID 0

RAID 0 (или как его еще называют Stripe). Режим, при использовании которого достигается максимальная производительность. Данные равномерно распределяются по дискам массива, диски объединяются в один, который может быть размечен на несколько. Распределенные операции чтения и записи позволяют значительно увеличить скорость работы, поскольку несколько дисков одновременно читают/записывают свою порцию данных. Пользователю доступен весь объем дисков, но это снижает надежность хранения данных, поскольку при отказе одного из дисков массив обычно разрушается и восстановить данные практически невозможно. Область применения - приложения, требующие высоких скоростей обмена с диском, например видеозахват, видеомонтаж. Рекомендуется использовать с высоконадежными дисками. 

RAID 1

RAID 1 (или Mirror). Несколько дисков (обычно 2), работающие синхронно на запись, то есть полностью дублирующие друг друга. Повышение производительности происходит только при чтении. Самый надежный способ защитить информацию от сбоя одного из дисков. Из-за высокой стоимости обычно используется при хранении важных данных. Высокая стоимость обусловлена тем, что лишь половина от общей емкости дисков доступна для пользователя.

RAID 10

RAID 10, также иногда называется RAID 1+0 - комбинация двух первых вариантов. (Массив RAID0 из массивов RAID1). Имеет все скоростные преимущества RAID0 и преимущество надежности RAID1, сохраняя недостаток - высокую стоимость дискового массива, так как эффективная ёмкость массива равна половине ёмкости использованных в нём дисков. Для создания такого массива требуется минимум 4 диска. (При этом их число должно быть чётным).

RAID 10 является достаточно надёжным вариантом для хранения данных в связи с тем, что весь массив RAID 10 будет выведен из строя только после выхода из строя всех накопителей в одном и том же массиве RAID 1. При одном вышедшем из строя накопителе, шанс выхода из строя второго в одном и том же массиве равен 33%.

Мы рассмотрели наиболее часто используемые типы RAID. Существует еще достаточно большое количество их вариаций, но мы их рассматривать не будем.

Так что же из себя представляют аппаратные RAID-контроллеры?

Это платы расширения, они размещаются либо в составе сервера либо вне сервера (например, в составе внешней дисковой подсистемы). Имеют собственный процессор, многие имеют кэш-память для ускорения работы. В устройствах подороже опционально устанавливаются батареи (химические, либо конденсаторные) для сохранения данных в кэше в случае аварийного отключения электропитания. Конденсаторные батареи более современные, но и более дорогие, поскольку дополнительно требуют наличия модуля энергонезависимой FLASH-памяти, куда при аварии будет копироваться кэш. Такие батареи не портятся со временем и, в отличие от химических, не требуют замены в течение срока службы сервера.

Для подключения дисков контроллер может иметь внутренние, либо внешние порты, либо и те, и другие. Порты могут быть выполнены по различным стандартам.

Контроллеры различных производителей, как правило, не совместимы и не взаимозаменяемы между собой — это следует иметь в виду в случае выхода из строя платы контроллера. Информация о конфигурации RAID-массива хранится на дисках, но прочитать её, даже с полностью исправных дисков, и воссоздать массив сможет только контроллер того же производителя. Для предотвращения подобных проблем существуют кластерные дисковые системы. Программные RAID-массивы также лишены этого недостатка (например mdadm RAID). Также некоторые RAID-контроллеры поддерживают такие дополнительные функции как  Горячая замена и резерв, а так же проверка на стабильность.

Adaptec

Что бы начать работу с RAID-контроллером Adaptec первое что необходимо установить библиотеки и утилиту arcconf.

Включаем smart и заносим конфигурацию в файл smartd.conf

Что бы увидеть текущее состояние RAID вы можете воспользоваться командой arrconf getconfig. Как вы видите она возвращает достаточно много полезной информации по вашему RAID массиву, уровень RAID, количество места, состояние кэшей и так далее.

Состояние самого жесткого диска можно получить все той же командой но используя другие аргументы. Отсюда мы также можем получить много полезной информации, модель и серийный номер жесткого диска, его объем, информация по SMART и кэшам. Также информация полученная с помощью данной команды используется при создании RAID, а именно Report Channel и Device

Создание RAID массива выполняется одной командой, ее вы можете видеть на слайде вместе с расшифровкой.

Как видите никаких сложностей в работе с данным контроллером нет, все создание рейда максимально быстрое и простое.

Переходим к RAID-контроллеру LSI MegaRAID.

Для работы с RAID-контроллером нам потребуется утилита megacli, команды для ее установки вы можете видеть на слайде. 

Теперь посмотрим как создать RAID10 на данном контроллере, первое что необходимо сделать посмотреть текущее состояние RAID, если нам достался старый сервер вполне возможно что RAID там может быть уже собран.

Перед настройкой массива, возможно, потребуется удалить использованную ранее конфигурацию. Для того чтобы просто удалить логические устройства вы можете использовать команду config LD Del

Для того чтобы удалить всё (в том числе, например, поведение кэша) используйте "Очистку конфигурации"

После того как мы выполнили чистку создаем RAID 10.

И включаем smart.

Как видим работа с MegaRAID также достаточно простая задача и вся настройка сводиться к нескольким командам.

Также хотелось бы немного затронуть тему настройки RAID контроллеров для лучшей производительности зачастую все настройки сводятся к работе с параметрами кэширования. Есть два основных типа кэша это Read кэш, то есть кэш на чтение и Write кэш на запись.

Кэш чтения
Позволяет кэшу контроллера читать данные вперёд и сохранять предпологаемые данные в кэш. Со включенным кэшом контроллер мониторит процесс чтений данных с пула дисков. Если обнаруживается определённый шаблон, контроллер заранее загружает следующие данные в кэш.

Кэш записи.
В режиме "write-back" контроллер посылает операционной системе подтверждение о завершённой записи и только тогда пишет данные на диск. Такой режим улучшает производительность, но может привести к потери данных в случае отказа электропитания системы.

Если к контроллеру подключена полностью заряженная батарейная или флэш-модуль, то кэш контроллера остаётся работоспособным около 72 часов. В это время необходимо восстановить электропитание системы, чтобы контроллер смог перенести данные из кэша контроллера на диски.

Если безопасность данных имеет высочайший приоритет и контроллер не имеет батарею или флэш-модуль, то не рекомендуется включать кэш записи контроллера. В таком случае выбирайте опцию "Disabled". Когда кэш записи на контроллере выключен, контроллер пишет сначала данные на диски и только потом посылает операционной системе подтверждение о том, что запись данных завершена. Таким образом падает производительность RAID-массива, но исключается вероятность потери данных.

Теперь посмотрим как включить кэширование на примере реального аппаратного рейд контроллера. В нашем случае это LSI MegaRAID.

Сначала нужно проверить состояние RAID

Далее нужно проверить наличие батарейки и ее состояние.

Если все порядке, включаем Cache без BBU, если нет - выключаем

Включаем Read Cache (данный кэш надо включать для достижения большей производительности системы ввода/вывода)

И наконец, включаем дисковый кэш